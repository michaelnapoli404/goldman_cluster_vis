# Wave Visualizer Package - Comprehensive Documentation

## Overview
A comprehensive Python package for visualizing longitudinal survey data transitions across multiple waves, with robust data preprocessing and customizable visualization options.

## Table of Contents
1. [Package Structure](#package-structure)
2. [Settings System Architecture](#settings-system-architecture)
3. [API Reference](#api-reference)
4. [Data Preparation](#data-preparation)
5. [Advanced Configuration](#advanced-configuration)
6. [Architecture Details](#architecture-details)
7. [Development Guide](#development-guide)
8. [Testing](#testing)
9. [Troubleshooting](#troubleshooting)

## Package Structure
```
wave_visualizer/
├── __init__.py
├── settings/
│   ├── __init__.py
│   ├── processed_data.csv
│   ├── metadata_output/
│   │   ├── variable_labels.csv
│   │   └── value_labels.csv
│   ├── cleaning_settings/
│   │   ├── missing_value_settings.csv
│   │   └── value_merging_settings.csv
│   └── visualization_settings/
│       ├── wave_definitions.csv
│       └── value_color_mappings.csv
├── data_prep/
│   ├── __init__.py
│   ├── cleaning/
│   │   ├── __init__.py
│   │   ├── metadata_handler.py
│   │   ├── values_to_labels.py
│   │   ├── value_missing_and_dropping_handler.py
│   │   ├── value_merging_handler.py
│   │   └── cleaning.py
│   ├── customization.py
│   ├── color_mapping.py
│   ├── wave_parser.py
│   └── export_handler.py
├── visualization_techs/
│   ├── __init__.py
│   ├── alluvial_plots.py
│   ├── alluvial_builder.py
│   ├── heatmaps.py
│   └── transition_pattern_analysis.py
├── utils/
│   ├── __init__.py
│   └── logger.py
├── config/
│   ├── __init__.py
│   └── package_config.py
├── exceptions.py
├── validators.py
└── interfaces.py
```

## Settings System Architecture

### **Three Categories of Settings**

The wave visualizer uses a settings system with three distinct categories based on how they are created and managed:

#### **GENERATED BY PYTHON** 
*Files created automatically from your data - no user input required*

**metadata_output/variable_labels.csv**
- Created by: `metadata_handler.py` 
- Purpose: Maps SPSS variable names to their descriptive labels
- Contains: Variable name, Variable label, Variable type
- Updated: Every time you run the data cleaning pipeline
- User action: None required

**metadata_output/value_labels.csv**
- Created by: `metadata_handler.py`
- Purpose: Maps numeric codes to their descriptive labels for categorical variables
- Contains: Variable name, Original value (numeric), Value label (text)
- Updated: Every time you run the data cleaning pipeline
- User action: None required

#### **GUIDED BY PYTHON WITH USER INPUT**
*Files created by Python with user configuration during interactive data cleaning*

**cleaning_settings/missing_value_settings.csv**
- Created by: `value_missing_and_dropping_handler.py`
- Purpose: Stores user decisions about which values to treat as missing/invalid
- Contains: Variable name, Values to treat as missing, Action (drop/keep)
- Updated: When you run interactive data cleaning
- User action: During cleaning, you specify which values should be treated as missing

**cleaning_settings/value_merging_settings.csv**
- Created by: `value_merging_handler.py`
- Purpose: Stores user decisions about combining similar response categories
- Contains: Variable name, Original values, Merged value name
- Updated: When you run interactive data cleaning
- User action: During cleaning, you specify which categories to combine

#### **USER-CONFIGURABLE CSV FILES**
*Files you can directly edit to customize behavior*

**visualization_settings/wave_definitions.csv**
- Purpose: Defines available survey waves and their column prefixes
- Columns: wave_number, column_prefix, description
- Example content:
  ```
  wave_number,column_prefix,description
  1,W1_,First wave of data collection
  2,W2_,Second wave of data collection  
  3,W3_,Third wave of data collection
  ```
- User action: Add rows for additional waves (W4, W5, etc.)

**visualization_settings/value_color_mappings.csv**
- Purpose: Assigns specific colors to categorical values for consistent visualization
- Columns: variable_name, value_name, color_hex, description
- Example content:
  ```
  variable_name,value_name,color_hex,description
  PID1_labeled,Republican,#FF0000,Red for Republican party
  PID1_labeled,Democrat,#0000FF,Blue for Democratic party
  PID1_labeled,Independent,#808080,Gray for Independent
  ```
- User action: Add color mappings for your variables and values

**processed_data.csv**
- Purpose: The cleaned and processed dataset ready for visualization
- Created by: The data cleaning pipeline
- Contains: All your survey data with proper labeling and formatting
- User action: Generate through the cleaning pipeline or provide your own cleaned data

### **How to Use Each Type**

1. **Auto-generated files**: Just run your data cleaning - these are created automatically
2. **User-guided files**: Run the interactive data cleaning pipeline and make decisions when prompted
3. **User-configurable files**: Edit directly with a text editor or Excel, or use the provided Python functions

### **Python Functions for Settings Management**

```python
# Add wave definitions
wave_visualizer.add_wave_definition('W4', 'W4_', 'Fourth wave data')

# Add color mappings  
wave_visualizer.add_color_mapping('PID1_labeled', 'Republican', '#FF0000')

# Check current settings
wave_visualizer.get_available_waves()
wave_visualizer.get_color_mappings('PID1_labeled')
```

## API Reference

### Core Visualization Functions

#### `create_alluvial_visualization()`
Create alluvial (Sankey-style) transition visualizations.

**Signature:**
```python
create_alluvial_visualization(
    data: Optional[pd.DataFrame] = None,
    variable_name: str = 'HFClust_labeled',
    wave_config: str = 'w1_to_w2',
    filter_column: Optional[str] = None,
    filter_value: Optional[str] = None,
    custom_title: Optional[str] = None,
    show_plot: bool = True,
    **kwargs
) -> Tuple[go.Figure, Dict[str, Any]]
```

**Parameters:**
- `data` (DataFrame, optional): Pre-cleaned dataset. If None, loads from `processed_data.csv`
- `variable_name` (str): Variable to analyze (e.g., 'HFClust_labeled')
- `wave_config` (str): Wave transition (e.g., 'w1_to_w2', 'w1_to_w3', 'w2_to_w3')
- `filter_column` (str, optional): Column to filter by for subset analysis
- `filter_value` (str, optional): Value to filter for (used with filter_column)
- `custom_title` (str, optional): Override automatic title generation
- `show_plot` (bool): Whether to display the plot immediately
- `**kwargs`: Additional arguments passed to the visualization builder

**Returns:**
- `fig` (plotly.Figure): Interactive visualization object
- `stats` (dict): Summary statistics including:
  - `total_transitions`: Number of valid transitions
  - `stability_rate`: Percentage of unchanged responses
  - `top_patterns`: Most common transition patterns
  - `missing_data_rate`: Percentage of missing data

**Example:**
```python
# Basic usage
fig, stats = wave_visualizer.create_alluvial_visualization(
    variable_name='HFClust_labeled',
    wave_config='w1_to_w3'
)

# With filtering
fig, stats = wave_visualizer.create_alluvial_visualization(
    variable_name='HFClust_labeled',
    wave_config='w1_to_w3',
    filter_column='PID1_labeled',
    filter_value='Republican',
    custom_title='Republican Voter Transitions: Health & Finance'
)
```

#### `export_figure()`
Export visualizations in multiple formats.

**Signature:**
```python
export_figure(
    fig: go.Figure,
    filename: str,
    formats: Optional[List[str]] = None
) -> Dict[str, str]
```

**Parameters:**
- `fig` (plotly.Figure): Figure to export
- `filename` (str): Base filename (without extension)
- `formats` (list, optional): Export formats ['html', 'png', 'svg', 'pdf']. Default: ['html', 'png']

**Returns:**
- `dict`: Mapping of format to file path for each exported file

**Example:**
```python
# Export to default formats (HTML + PNG)
paths = wave_visualizer.export_figure(fig, 'republican_analysis')

# Export to specific formats
paths = wave_visualizer.export_figure(
    fig, 
    'detailed_analysis', 
    ['html', 'png', 'svg', 'pdf']
)
```

### Configuration Functions

#### `add_color_mapping()`
Add semantic color mapping for consistent visualization colors.

**Signature:**
```python
add_color_mapping(
    variable_name: str,
    value_name: str,
    color_hex: str,
    description: str = ""
) -> None
```

**Parameters:**
- `variable_name`: Name of the variable (e.g., 'PID1_labeled')
- `value_name`: Specific value to color (e.g., 'Republican')
- `color_hex`: Hex color code (e.g., '#FF0000')
- `description`: Optional description for documentation

#### `add_wave_definition()`
Add support for additional survey waves.

**Signature:**
```python
add_wave_definition(
    wave_name: str,
    column_prefix: str,
    description: str = ""
) -> None
```

**Parameters:**
- `wave_name`: Wave identifier (e.g., 'W4')
- `column_prefix`: Column prefix in data (e.g., 'W4_')
- `description`: Optional description

#### `get_available_waves()`
Get list of currently supported wave numbers.

**Returns:**
- `list`: Available wave numbers (e.g., [1, 2, 3, 4])

#### `configure_package_logging()`
Configure package-wide logging behavior.

**Signature:**
```python
configure_package_logging(
    level: str = 'INFO',
    log_file: Optional[str] = None,
    quiet: bool = False
) -> None
```

**Parameters:**
- `level`: Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')
- `log_file`: Optional log file path
- `quiet`: Suppress console output if True

### Data Cleaning Functions

#### `DataCleaningPipeline.run_full_pipeline()`
Run the complete data cleaning workflow.

**Signature:**
```python
run_full_pipeline(
    data_file_path: str,
    interactive: bool = True
) -> bool
```

**Parameters:**
- `data_file_path`: Path to SPSS (.sav) file
- `interactive`: Enable interactive prompts for user decisions

**Returns:**
- `bool`: Success status

## Data Preparation

### Input Data Requirements

Your SPSS data should have:
- **Wave-prefixed columns**: Variables named like `W1_variable`, `W2_variable`
- **Labeled categorical variables**: SPSS value labels for categorical responses
- **Consistent naming**: Same variable names across waves (just different prefixes)
- **Complete metadata**: Variable labels and value labels in SPSS file

### Cleaning Pipeline Steps

1. **Metadata Extraction**: Extract variable and value labels from SPSS
2. **Values to Labels**: Convert numeric codes to descriptive text
3. **Missing Value Handling**: Identify and handle missing/invalid responses
4. **Value Merging**: Combine similar response categories if needed
5. **Final Processing**: Create analysis-ready dataset

### Running the Cleaning Pipeline

#### Interactive Mode (Recommended)
```python
from wave_visualizer.data_prep.cleaning import DataCleaningPipeline

pipeline = DataCleaningPipeline()
success = pipeline.run_full_pipeline(
    data_file_path="your_data.sav",
    interactive=True
)
```

#### Non-Interactive Mode
```python
# Requires pre-configured settings files
success = pipeline.run_full_pipeline(
    data_file_path="your_data.sav",
    interactive=False
)
```

## Advanced Configuration

### Logging Configuration

```python
# Detailed logging for development
wave_visualizer.configure_package_logging(
    level='DEBUG',
    log_file='development.log',
    quiet=False
)

# Production logging
wave_visualizer.configure_package_logging(
    level='WARNING',
    log_file='production.log',
    quiet=True
)
```

### Export Configuration

```python
# High-resolution exports
wave_visualizer.configure_package(
    figure_width=1600,
    figure_height=900,
    image_scale=4,  # 4x resolution
    default_formats=['html', 'png', 'svg']
)
```

### Custom Color Schemes

```python
# Political party colors
political_colors = {
    'Republican': '#D62728',  # Red
    'Democrat': '#1F77B4',    # Blue  
    'Independent': '#2CA02C', # Green
    'Other': '#FF7F0E'        # Orange
}

for party, color in political_colors.items():
    wave_visualizer.add_color_mapping('PID1_labeled', party, color)
```

## Architecture Details

### Design Patterns

Wave Visualizer follows professional software engineering practices:

- **Builder Pattern**: `AlluvialVisualizationBuilder` for modular construction
- **Strategy Pattern**: Pluggable data cleaning strategies  
- **Factory Pattern**: `VisualizationFactory` for flexible component creation
- **Observer Pattern**: Progress monitoring and logging
- **Command Pattern**: `DataProcessingCommand` for undoable operations

### Key Interfaces

#### `VisualizationBuilder`
Protocol for visualization construction:
```python
class VisualizationBuilder(Protocol):
    def set_data(self, data: pd.DataFrame) -> 'VisualizationBuilder': ...
    def set_variable(self, variable_name: str) -> 'VisualizationBuilder': ...
    def build(self) -> Tuple[go.Figure, Dict[str, Any]]: ...
```

#### `DataProcessor`
Protocol for data processing components:
```python
class DataProcessor(Protocol):
    def process(self, data: pd.DataFrame) -> pd.DataFrame: ...
    def validate_input(self, data: pd.DataFrame) -> bool: ...
```

### Component Responsibilities

- **`visualization_techs/`**: Visualization generation and rendering
- **`data_prep/`**: Data cleaning, preprocessing, and configuration
- **`validators/`**: Input validation and sanitization
- **`config/`**: Configuration management and persistence
- **`exceptions/`**: Custom exception hierarchy with helpful messages
- **`utils/`**: Logging, utilities, and helper functions

## Development Guide

### Environment Setup

```bash
# Clone repository
git clone https://github.com/michaelnapoli404/wave-visualizer.git
cd wave-visualizer

# Setup development environment
make setup-dev

# Verify installation
python -c "import wave_visualizer; print('Success!')"
```

### Development Workflow

```bash
# Format code
make format

# Run linting
make lint

# Type checking
make type-check

# Security scan
make security

# Run all quality checks
make all-checks

# Quick development check
make quick
```

### Code Quality Standards

- **Formatting**: Black with 88-character lines
- **Import sorting**: isort with Black compatibility
- **Type checking**: mypy with strict mode
- **Linting**: flake8 with comprehensive rules
- **Security**: bandit security scanning
- **Documentation**: Google-style docstrings

### Pre-commit Hooks

```bash
# Install pre-commit hooks
make pre-commit-install

# Run hooks manually
make pre-commit
```

## Testing

### Test Categories

- **Unit tests**: Individual component testing
- **Integration tests**: Component interaction testing
- **API tests**: Public interface validation
- **End-to-end tests**: Complete workflow testing

### Running Tests

```bash
# All tests
make test

# With coverage report
make test-cov

# Fast tests only
make test-fast

# Specific categories
pytest -m unit
pytest -m integration
pytest -m "not slow"
```

### Test Structure

```
tests/
├── conftest.py          # Shared fixtures
├── test_validators.py   # Validation testing
├── test_alluvial_builder.py  # Builder pattern testing
├── test_integration.py  # Integration testing
└── helpers/             # Test utilities
```

## Troubleshooting

### Common Issues

#### Missing Data Error
```
FileNotFoundError: processed_data.csv not found
```
**Solution**: Run the data cleaning pipeline first:
```python
from wave_visualizer.data_prep.cleaning import DataCleaningPipeline
pipeline = DataCleaningPipeline()
pipeline.run_full_pipeline("your_data.sav", interactive=True)
```

#### Column Not Found Error
```
ColumnNotFoundError: Column 'W4_HFClust_labeled' not found
```
**Solution**: Add wave definition for W4:
```python
wave_visualizer.add_wave_definition('W4', 'W4_', 'Fourth wave')
```

#### Import Error
```
ModuleNotFoundError: No module named 'wave_visualizer'
```
**Solution**: Install package in development mode:
```bash
pip install -e .
```

#### Empty Visualization
```
No data found after filtering
```
**Solution**: Check filter values and data:
```python
# Check available values
print(data['filter_column'].value_counts())

# Verify data has required columns
print(data.columns.tolist())
```

### Debug Mode

Enable detailed logging for troubleshooting:
```python
wave_visualizer.configure_package_logging(level='DEBUG')
```

### Performance Tips

1. **Large datasets**: Use filtering to reduce data size
2. **Multiple visualizations**: Reuse loaded data
3. **Export optimization**: Choose appropriate image formats and resolutions
4. **Memory usage**: Clear figure objects when done

### Getting Help

1. Check this documentation for detailed explanations
2. Run with DEBUG logging to see detailed execution
3. Check the `example_visualizations/` folder for working examples
4. Review test files for usage patterns
5. Open GitHub issues for bugs or feature requests

---

**Note**: This documentation covers version 0.1.0 of Wave Visualizer. For the latest updates, see CHANGELOG.md. 